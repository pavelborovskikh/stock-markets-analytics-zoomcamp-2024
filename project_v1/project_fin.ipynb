{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Setup"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import talib\n",
    "\n",
    "#Fin Data Sources\n",
    "import yfinance as yf\n",
    "import pandas_datareader as pdr\n",
    "\n",
    "# ML models and utils\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "import time\n",
    "from datetime import date, datetime, timedelta\n",
    "\n",
    "# Disable SettingWithCopyWarning\n",
    "pd.options.mode.chained_assignment = None  # default='warn'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load Data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Define the start and end dates\n",
    "## Define the end date as the yesterday's date\n",
    "end = date.today() # today\n",
    "end1 = date.today() - timedelta(days=1) # yesterday\n",
    "#print(f'Year = {end.year}; month= {end.month}; day={end.day}')\n",
    "\n",
    "start = date(year=end.year-70, month=end.month, day=end.day)\n",
    "#print(f'Period for data: {start} to {end} ')\n",
    "#print(f'Period for data: {start} to {end1} ')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Macro"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Germany"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Define the function to get a data frame with \"Growth\" variables\n",
    "def get_growth_df(df:pd.DataFrame, prefix:str)->pd.DataFrame:\n",
    "  for i in [1,3,7,30,90,365]:\n",
    "    df['growth_'+prefix+'_'+str(i)+'d'] = df['Adj Close'] / df['Adj Close'].shift(i)\n",
    "    GROWTH_KEYS = [k for k in df.keys() if k.startswith('growth')]\n",
    "  return df[GROWTH_KEYS]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# FRED's data for germany: https://fred.stlouisfed.org/categories/32273"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# get the quarterly GDP data for Germany from Fred (https://fred.stlouisfed.org/series/NGDPRSAXDCDEQ)\n",
    "gdp = pdr.DataReader(\"NGDPRSAXDCDEQ\", \"fred\", start=start, end=end)\n",
    "\n",
    "gdp['gdp_de_yoy'] = gdp.NGDPRSAXDCDEQ/gdp.NGDPRSAXDCDEQ.shift(4)-1\n",
    "gdp['gdp_de_qoq'] = gdp.NGDPRSAXDCDEQ/gdp.NGDPRSAXDCDEQ.shift(1)-1\n",
    "\n",
    "gdp_to_merge = gdp[['gdp_de_yoy','gdp_de_qoq']]\n",
    "#gdp_to_merge.info()\n",
    "#gdp_to_merge.tail()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# CPI (https://fred.stlouisfed.org/series/DEUCPIALLMINMEI)\n",
    "cpi = pdr.DataReader(\"DEUCPIALLMINMEI\", \"fred\", start=start, end=end)\n",
    "cpi['cpi_de_yoy'] = cpi.DEUCPIALLMINMEI/cpi.DEUCPIALLMINMEI.shift(12)-1\n",
    "cpi['cpi_de_mom'] = cpi.DEUCPIALLMINMEI/cpi.DEUCPIALLMINMEI.shift(1)-1\n",
    "\n",
    "cpi_to_merge = cpi[['cpi_de_yoy','cpi_de_mom']]\n",
    "#cpi_to_merge.info()\n",
    "#cpi_to_merge.tail()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Interest Rates: Long-Term Government Bond Yields: 10-Year: Main (Including Benchmark) for Germany (https://fred.stlouisfed.org/series/IRLTLT01DEM156N)\n",
    "bond10 = pdr.DataReader(\"IRLTLT01DEM156N\", \"fred\", start=start, end=end).rename(columns={'IRLTLT01DEM156N':'bond10'})\n",
    "#bond10.info()\n",
    "#bond10.tail()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Other"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# GOLD\n",
    "# WEB: https://finance.yahoo.com/quote/GC%3DF\n",
    "gold = yf.download(tickers = \"GC=F\",\n",
    "                     #period = \"max\",\n",
    "                    start = start,\n",
    "                     end = end,\n",
    "                     interval = \"1d\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "gold_to_merge = get_growth_df(gold,'gold')\n",
    "#gold_to_merge.tail(2)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# WTI Crude Oil\n",
    "# WEB: https://uk.finance.yahoo.com/quote/CL=F/\n",
    "crude_oil = yf.download(tickers = \"CL=F\",\n",
    "                     #period = \"max\",\n",
    "                    start = start,\n",
    "                    end = end,\n",
    "                     interval = \"1d\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "crude_oil_to_merge = get_growth_df(crude_oil,'wti_oil')\n",
    "#crude_oil_to_merge.tail(2)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Brent Oil\n",
    "# WEB: https://uk.finance.yahoo.com/quote/BZ=F/\n",
    "brent_oil = yf.download(tickers = \"BZ=F\",\n",
    "                     #period = \"max\",\n",
    "                    start = start,\n",
    "                    end = end,\n",
    "                     interval = \"1d\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "brent_oil_to_merge = get_growth_df(brent_oil,'brent_oil')\n",
    "#brent_oil_to_merge.tail(2)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# https://finance.yahoo.com/quote/BTC-USD/\n",
    "btc_usd =  yf.download(tickers = \"BTC-USD\",\n",
    "                     #period = \"max\",\n",
    "                    start = start,\n",
    "                    end = end,\n",
    "                    interval = \"1d\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "btc_usd_to_merge = get_growth_df(btc_usd,'btc_usd')\n",
    "#btc_usd_to_merge.tail(2)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Equities"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Indexes"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# get the DAX data from Yahoo Finance\n",
    "dax_daily = yf.download(tickers = \"^GDAXI\",\n",
    "                     #period = \"max\",\n",
    "                    start = start,\n",
    "                     end = end,\n",
    "                     interval = \"1d\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# get a DAX data frame with \"Growth\" variables\n",
    "df_dax_to_merge = get_growth_df(dax_daily, \"dax\")\n",
    "#df_dax_to_merge.info()\n",
    "#df_dax_to_merge.tail()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# https://finance.yahoo.com/quote/%5EGSPC/\n",
    "# SNP - SNP Real Time Price. Currency in USD\n",
    "snp500_daily = yf.download(tickers = \"^GSPC\",\n",
    "                     #period = \"max\",\n",
    "                    start = start,\n",
    "                     end = end,\n",
    "                     interval = \"1d\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "snp500_to_merge = get_growth_df(snp500_daily,'snp500')\n",
    "#snp500_to_merge.tail(2)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Dow Jones Industrial Average: https://finance.yahoo.com/quote/%5EDJI?.tsrc=fin-srch\n",
    "dji_daily = yf.download(tickers = \"^DJI\",\n",
    "                     #period = \"max\",\n",
    "                    start = start,\n",
    "                     end = end,\n",
    "                     interval = \"1d\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dji_daily_to_merge = get_growth_df(dji_daily,'dji')\n",
    "#dji_daily_to_merge.tail(2)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# VIX - Volatility Index\n",
    "# https://finance.yahoo.com/quote/%5EVIX/\n",
    "vix = yf.download(tickers = \"^VIX\",\n",
    "                     #period = \"max\",\n",
    "                    start = start,\n",
    "                     end = end,\n",
    "                     interval = \"1d\").rename(columns={'Adj Close':'vix_adj_close'})\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#vix.tail(2)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "vix_to_merge = vix['vix_adj_close']\n",
    "vix_to_merge.tail()\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Stocks"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# DE_STOCKS\n",
    "# https://companiesmarketcap.com/germany/largest-companies-in-germany-by-market-cap/\n",
    "DE_STOCKS = [\"SAP\", \"SIE.DE\", \"DTE.DE\", \"ALV.DE\", \"P911.DE\", \"MBG.DE\", \"MRK.DE\", \"MUV2.DE\", \"SHL.DE\", \"BMW.DE\", \"VOW3.DE\"]\n",
    "# DE_STOCKS_ADD = [\"ADS.DE\", \"BAS.DE\", \"BAYN.DE\", \"BEI.DE\", \"CON.DE\", \"1COV.DE\", \"DAI.DE\", \"DBK.DE\", \"DB1.DE\", \"DPW.DE\", \"DWNI.DE\", \"EOAN.DE\", \"FRE.DE\", \"FME.DE\", \"HEI.DE\", \"HEN3.DE\", \"IFX.DE\", \"LIN.DE\", \"MTX.DE\", \"RWE.DE\", \"VNA.DE\", \"WDI.DE\"]\n",
    "\n",
    "# EU_STOCKS\n",
    "# https://companiesmarketcap.com/european-union/largest-companies-in-the-eu-by-market-cap/\n",
    "EU_STOCKS = ['NVO', 'MC.PA', 'ASML', 'RMS.PA', 'OR.PA', 'ACN', 'TTE', 'PRX.AS', 'IDEXY', 'SU.PA', 'ETN']\n",
    "\n",
    "# US_STOCKS\n",
    "# https://companiesmarketcap.com/usa/largest-companies-in-the-usa-by-market-cap/\n",
    "US_STOCKS = ['MSFT', 'AAPL', 'GOOG', 'NVDA', 'AMZN', 'META', 'BRK-B', 'LLY', 'TSLA', 'AVGO', 'JPM']\n",
    "\n",
    "# CN_STOCKS?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ALL_TICKERS = DE_STOCKS + EU_STOCKS + US_STOCKS\n",
    "#print(ALL_TICKERS)\n",
    "#len(ALL_TICKERS)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "stocks_df = pd.DataFrame({'A' : []})\n",
    "\n",
    "for i,ticker in enumerate(ALL_TICKERS):\n",
    "  print(i,ticker)\n",
    "\n",
    "  # Work with stock prices\n",
    "  historyPrices = yf.download(tickers = ticker,\n",
    "                     #period = \"max\",\n",
    "                    start = start,\n",
    "                     end = end,\n",
    "                     interval = \"1d\")\n",
    "\n",
    "  # generate features for historical prices, and what we want to predict\n",
    "  historyPrices['Ticker'] = ticker\n",
    "  historyPrices['Year']= historyPrices.index.year\n",
    "  historyPrices['Month'] = historyPrices.index.month\n",
    "  historyPrices['Weekday'] = historyPrices.index.weekday\n",
    "  historyPrices['Date'] = historyPrices.index.date\n",
    "\n",
    "  # historical returns\n",
    "  for i in [1,3,7,30,90,365]:\n",
    "    historyPrices['growth_'+str(i)+'d'] = historyPrices['Adj Close'] / historyPrices['Adj Close'].shift(i)\n",
    "  #historyPrices['growth_future_5d'] = historyPrices['Adj Close'].shift(-5) / historyPrices['Adj Close']\n",
    "  historyPrices['growth_future_3d'] = historyPrices['Adj Close'].shift(-3) / historyPrices['Adj Close']\n",
    "\n",
    "  # Technical indicators\n",
    "  # SimpleMovingAverage 10 days and 20 days\n",
    "  historyPrices['SMA10']= historyPrices['Close'].rolling(10).mean()\n",
    "  historyPrices['SMA20']= historyPrices['Close'].rolling(20).mean()\n",
    "  historyPrices['growing_moving_average'] = np.where(historyPrices['SMA10'] > historyPrices['SMA20'], 1, 0)\n",
    "  historyPrices['high_minus_low_relative'] = (historyPrices.High - historyPrices.Low) / historyPrices['Adj Close']\n",
    "\n",
    "  # 30d rolling volatility : https://ycharts.com/glossary/terms/rolling_vol_30\n",
    "  historyPrices['volatility'] =   historyPrices['Adj Close'].rolling(30).std() * np.sqrt(252)\n",
    "\n",
    "  # what we want to predict\n",
    "  #historyPrices['is_positive_growth_5d_future'] = np.where(historyPrices['growth_future_5d'] > 1, 1, 0)\n",
    "  historyPrices['is_positive_growth_3d_future'] = np.where(historyPrices['growth_future_3d'] > 1, 1, 0)\n",
    "\n",
    "  # sleep 1 sec between downloads - not to overload the API server\n",
    "  time.sleep(1)\n",
    "\n",
    "\n",
    "  if stocks_df.empty:\n",
    "    stocks_df = historyPrices\n",
    "  else:\n",
    "    stocks_df = pd.concat([stocks_df, historyPrices], ignore_index=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_ticker_type(ticker:str, us_stocks_list, eu_stocks_list, de_stocks_list):\n",
    "  if ticker in us_stocks_list:\n",
    "    return 'US'\n",
    "  elif ticker in eu_stocks_list:\n",
    "    return 'EU'\n",
    "  elif ticker in de_stocks_list:\n",
    "    return 'DE'\n",
    "  else:\n",
    "    return 'ERROR'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "stocks_df['ticker_type'] = stocks_df.Ticker.apply(lambda x:get_ticker_type(x, US_STOCKS, EU_STOCKS, DE_STOCKS))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# count of observations between US-EU-INDIA stocks\n",
    "#stocks_df.ticker_type.value_counts()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# unique tickers\n",
    "#stocks_df.Ticker.nunique()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# count of observations by stock\n",
    "#stocks_df.Ticker.value_counts()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#stocks_df.groupby(['Ticker','ticker_type']).Date.agg(['min','max'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#  average growth 365days\n",
    "#stocks_df[stocks_df.Year>=2020].groupby(by=['Year','ticker_type']).growth_365d.mean()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "stocks_df['Date'] = pd.to_datetime(stocks_df['Date'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# filtering only on stats after 2000\n",
    "#stocks_df[stocks_df.Date>='2000-01-01'].info()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Technical indicators"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Momentum indicators\n",
    "https://github.com/TA-Lib/ta-lib-python/blob/master/docs/func_groups/momentum_indicators.md"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def talib_get_momentum_indicators_for_one_ticker(df: pd.DataFrame) -> pd.DataFrame:\n",
    "  # ADX - Average Directional Movement Index\n",
    "  talib_momentum_adx = talib.ADX(df.High.values, df.Low.values, df.Close.values, timeperiod=14)\n",
    "  # ADXR - Average Directional Movement Index Rating\n",
    "  talib_momentum_adxr = talib.ADXR(df.High.values, df.Low.values, df.Close.values, timeperiod=14 )\n",
    "  # APO - Absolute Price Oscillator\n",
    "  talib_momentum_apo = talib.APO(df.Close.values, fastperiod=12, slowperiod=26, matype=0 )\n",
    "  # AROON - Aroon\n",
    "  talib_momentum_aroon = talib.AROON(df.High.values, df.Low.values, timeperiod=14 )\n",
    "  # talib_momentum_aroon[0].size\n",
    "  # talib_momentum_aroon[1].size\n",
    "  # AROONOSC - Aroon Oscillator\n",
    "  talib_momentum_aroonosc = talib.AROONOSC(df.High.values, df.Low.values, timeperiod=14)\n",
    "  # BOP - Balance of Power\n",
    "  # https://school.stockcharts.com/doku.php?id=technical_indicators:balance_of_power\n",
    "     #calculate open prices as shifted closed prices from the prev day\n",
    "     # open = df.Last.shift(1)\n",
    "  talib_momentum_bop = talib.BOP(df.Open.values, df.High.values, df.Low.values, df.Close.values)\n",
    "  # CCI - Commodity Channel Index\n",
    "  talib_momentum_cci = talib.CCI(df.High.values, df.Low.values, df.Close.values, timeperiod=14)\n",
    "  # CMO - Chande Momentum Oscillator\n",
    "  talib_momentum_cmo = talib.CMO(df.Close.values, timeperiod=14)\n",
    "  # DX - Directional Movement Index\n",
    "  talib_momentum_dx = talib.DX(df.High.values, df.Low.values, df.Close.values, timeperiod=14)\n",
    "  # MACD - Moving Average Convergence/Divergence\n",
    "  talib_momentum_macd, talib_momentum_macdsignal, talib_momentum_macdhist = talib.MACD(df.Close.values, fastperiod=12, \\\n",
    "                                                                                       slowperiod=26, signalperiod=9)\n",
    "  # MACDEXT - MACD with controllable MA type\n",
    "  talib_momentum_macd_ext, talib_momentum_macdsignal_ext, talib_momentum_macdhist_ext = talib.MACDEXT(df.Close.values, \\\n",
    "                                                                                                    fastperiod=12, \\\n",
    "                                                                                                    fastmatype=0, \\\n",
    "                                                                                                    slowperiod=26, \\\n",
    "                                                                                                    slowmatype=0, \\\n",
    "                                                                                                    signalperiod=9, \\\n",
    "                                                                                                  signalmatype=0)\n",
    "  # MACDFIX - Moving Average Convergence/Divergence Fix 12/26\n",
    "  talib_momentum_macd_fix, talib_momentum_macdsignal_fix, talib_momentum_macdhist_fix = talib.MACDFIX(df.Close.values, \\\n",
    "                                                                                                      signalperiod=9)\n",
    "  # MFI - Money Flow Index\n",
    "  talib_momentum_mfi = talib.MFI(df.High.values, df.Low.values, df.Close.values, df.Volume.values, timeperiod=14)\n",
    "  # MINUS_DI - Minus Directional Indicator\n",
    "  talib_momentum_minus_di = talib.MINUS_DM(df.High.values, df.Low.values, timeperiod=14)\n",
    "  # MOM - Momentum\n",
    "  talib_momentum_mom = talib.MOM(df.Close.values, timeperiod=10)\n",
    "  # PLUS_DI - Plus Directional Indicator\n",
    "  talib_momentum_plus_di = talib.PLUS_DI(df.High.values, df.Low.values, df.Close.values, timeperiod=14)\n",
    "  # PLUS_DM - Plus Directional Movement\n",
    "  talib_momentum_plus_dm = talib.PLUS_DM(df.High.values, df.Low.values, timeperiod=14)\n",
    "  # PPO - Percentage Price Oscillator\n",
    "  talib_momentum_ppo = talib.PPO(df.Close.values, fastperiod=12, slowperiod=26, matype=0)\n",
    "  # ROC - Rate of change : ((price/prevPrice)-1)*100\n",
    "  talib_momentum_roc = talib.ROC(df.Close.values, timeperiod=10)\n",
    "  # ROCP - Rate of change Percentage: (price-prevPrice)/prevPrice\n",
    "  talib_momentum_rocp = talib.ROCP(df.Close.values, timeperiod=10)\n",
    "  # ROCR - Rate of change ratio: (price/prevPrice)\n",
    "  talib_momentum_rocr = talib.ROCR(df.Close.values, timeperiod=10)\n",
    "  # ROCR100 - Rate of change ratio 100 scale: (price/prevPrice)*100\n",
    "  talib_momentum_rocr100 = talib.ROCR100(df.Close.values, timeperiod=10)\n",
    "  # RSI - Relative Strength Index\n",
    "  talib_momentum_rsi = talib.RSI(df.Close.values, timeperiod=14)\n",
    "  # STOCH - Stochastic\n",
    "  talib_momentum_slowk, talib_momentum_slowd = talib.STOCH(df.High.values, df.Low.values, df.Close.values, \\\n",
    "                                                           fastk_period=5, slowk_period=3, slowk_matype=0, slowd_period=3, slowd_matype=0)\n",
    "  # STOCHF - Stochastic Fast\n",
    "  talib_momentum_fastk, talib_momentum_fastd = talib.STOCHF(df.High.values, df.Low.values, df.Close.values, \\\n",
    "                                                            fastk_period=5, fastd_period=3, fastd_matype=0)\n",
    "  # STOCHRSI - Stochastic Relative Strength Index\n",
    "  talib_momentum_fastk_rsi, talib_momentum_fastd_rsi = talib.STOCHRSI(df.Close.values, timeperiod=14, \\\n",
    "                                                                      fastk_period=5, fastd_period=3, fastd_matype=0)\n",
    "  # TRIX - 1-day Rate-Of-Change (ROC) of a Triple Smooth EMA\n",
    "  talib_momentum_trix = talib.TRIX(df.Close.values, timeperiod=30)\n",
    "  # ULTOSC - Ultimate Oscillator\n",
    "  talib_momentum_ultosc = talib.ULTOSC(df.High.values, df.Low.values, df.Close.values, timeperiod1=7, timeperiod2=14, timeperiod3=28)\n",
    "  # WILLR - Williams' %R\n",
    "  talib_momentum_willr = talib.WILLR(df.High.values, df.Low.values, df.Close.values, timeperiod=14)\n",
    "\n",
    "  momentum_df =   pd.DataFrame(\n",
    "    {\n",
    "      # assume here multi-index <dateTime, ticker>\n",
    "      # 'datetime': df.index.get_level_values(0),\n",
    "      # 'ticker': df.index.get_level_values(1) ,\n",
    "\n",
    "      # old way with separate columns\n",
    "      'Date': df.Date.values,\n",
    "      'Ticker': df.Ticker,\n",
    "\n",
    "      'adx': talib_momentum_adx,\n",
    "      'adxr': talib_momentum_adxr,\n",
    "      'apo': talib_momentum_apo,\n",
    "      'aroon_1': talib_momentum_aroon[0] ,\n",
    "      'aroon_2': talib_momentum_aroon[1],\n",
    "      'aroonosc': talib_momentum_aroonosc,\n",
    "      'bop': talib_momentum_bop,\n",
    "      'cci': talib_momentum_cci,\n",
    "      'cmo': talib_momentum_cmo,\n",
    "      'dx': talib_momentum_dx,\n",
    "      'macd': talib_momentum_macd,\n",
    "      'macdsignal': talib_momentum_macdsignal,\n",
    "      'macdhist': talib_momentum_macdhist,\n",
    "      'macd_ext': talib_momentum_macd_ext,\n",
    "      'macdsignal_ext': talib_momentum_macdsignal_ext,\n",
    "      'macdhist_ext': talib_momentum_macdhist_ext,\n",
    "      'macd_fix': talib_momentum_macd_fix,\n",
    "      'macdsignal_fix': talib_momentum_macdsignal_fix,\n",
    "      'macdhist_fix': talib_momentum_macdhist_fix,\n",
    "      'mfi': talib_momentum_mfi,\n",
    "      'minus_di': talib_momentum_minus_di,\n",
    "      'mom': talib_momentum_mom,\n",
    "      'plus_di': talib_momentum_plus_di,\n",
    "      'dm': talib_momentum_plus_dm,\n",
    "      'ppo': talib_momentum_ppo,\n",
    "      'roc': talib_momentum_roc,\n",
    "      'rocp': talib_momentum_rocp,\n",
    "      'rocr': talib_momentum_rocr,\n",
    "      'rocr100': talib_momentum_rocr100,\n",
    "      'rsi': talib_momentum_rsi,\n",
    "      'slowk': talib_momentum_slowk,\n",
    "      'slowd': talib_momentum_slowd,\n",
    "      'fastk': talib_momentum_fastk,\n",
    "      'fastd': talib_momentum_fastd,\n",
    "      'fastk_rsi': talib_momentum_fastk_rsi,\n",
    "      'fastd_rsi': talib_momentum_fastd_rsi,\n",
    "      'trix': talib_momentum_trix,\n",
    "      'ultosc': talib_momentum_ultosc,\n",
    "      'willr': talib_momentum_willr,\n",
    "     }\n",
    "  )\n",
    "  return momentum_df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Volume, Volatility, Cycle, Price indicators\n",
    "https://github.com/TA-Lib/ta-lib-python/blob/master/docs/func_groups/volume_indicators.md"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def talib_get_volume_volatility_cycle_price_indicators(df: pd.DataFrame) -> pd.DataFrame:\n",
    "        # TA-Lib Volume indicators\n",
    "        # https://github.com/TA-Lib/ta-lib-python/blob/master/docs/func_groups/volume_indicators.md\n",
    "        # AD - Chaikin A/D Line\n",
    "        talib_ad = talib.AD(\n",
    "            df.High.values, df.Low.values, df.Close.values, df.Volume.values)\n",
    "        # ADOSC - Chaikin A/D Oscillator\n",
    "        talib_adosc = talib.ADOSC(\n",
    "            df.High.values, df.Low.values, df.Close.values, df.Volume.values, fastperiod=3, slowperiod=10)\n",
    "        # OBV - On Balance Volume\n",
    "        talib_obv = talib.OBV(\n",
    "            df.Close.values, df.Volume.values)\n",
    "\n",
    "        # TA-Lib Volatility indicators\n",
    "        # https://github.com/TA-Lib/ta-lib-python/blob/master/docs/func_groups/volatility_indicators.md\n",
    "        # ATR - Average True Range\n",
    "        talib_atr = talib.ATR(\n",
    "            df.High.values, df.Low.values, df.Close.values, timeperiod=14)\n",
    "        # NATR - Normalized Average True Range\n",
    "        talib_natr = talib.NATR(\n",
    "            df.High.values, df.Low.values, df.Close.values, timeperiod=14)\n",
    "        # OBV - On Balance Volume\n",
    "        talib_obv = talib.OBV(\n",
    "            df.Close.values, df.Volume.values)\n",
    "\n",
    "        # TA-Lib Cycle Indicators\n",
    "        # https://github.com/TA-Lib/ta-lib-python/blob/master/docs/func_groups/cycle_indicators.md\n",
    "        # HT_DCPERIOD - Hilbert Transform - Dominant Cycle Period\n",
    "        talib_ht_dcperiod = talib.HT_DCPERIOD(df.Close.values)\n",
    "        # HT_DCPHASE - Hilbert Transform - Dominant Cycle Phase\n",
    "        talib_ht_dcphase = talib.HT_DCPHASE(df.Close.values)\n",
    "        # HT_PHASOR - Hilbert Transform - Phasor Components\n",
    "        talib_ht_phasor_inphase, talib_ht_phasor_quadrature = talib.HT_PHASOR(\n",
    "            df.Close.values)\n",
    "        # HT_SINE - Hilbert Transform - SineWave\n",
    "        talib_ht_sine_sine, talib_ht_sine_leadsine = talib.HT_SINE(\n",
    "            df.Close.values)\n",
    "        # HT_TRENDMODE - Hilbert Transform - Trend vs Cycle Mode\n",
    "        talib_ht_trendmode = talib.HT_TRENDMODE(df.Close.values)\n",
    "\n",
    "        # TA-Lib Price Transform Functions\n",
    "        # https://github.com/TA-Lib/ta-lib-python/blob/master/docs/func_groups/price_transform.md\n",
    "        # AVGPRICE - Average Price\n",
    "        talib_avgprice = talib.AVGPRICE(\n",
    "            df.Open.values, df.High.values, df.Low.values, df.Close.values)\n",
    "        # MEDPRICE - Median Price\n",
    "        talib_medprice = talib.MEDPRICE(df.High.values, df.Low.values)\n",
    "        # TYPPRICE - Typical Price\n",
    "        talib_typprice = talib.TYPPRICE(\n",
    "            df.High.values, df.Low.values, df.Close.values)\n",
    "        # WCLPRICE - Weighted Close Price\n",
    "        talib_wclprice = talib.WCLPRICE(\n",
    "            df.High.values, df.Low.values, df.Close.values)\n",
    "\n",
    "        volume_volatility_cycle_price_df = pd.DataFrame(\n",
    "            {'Date': df.Date.values,\n",
    "             'Ticker': df.Ticker,\n",
    "             # TA-Lib Volume indicators\n",
    "             'ad': talib_ad,\n",
    "             'adosc': talib_adosc,\n",
    "             'obv': talib_obv,\n",
    "             # TA-Lib Volatility indicators\n",
    "             'atr': talib_atr,\n",
    "             'natr': talib_natr,\n",
    "             'obv': talib_obv,\n",
    "             # TA-Lib Cycle Indicators\n",
    "             'ht_dcperiod': talib_ht_dcperiod,\n",
    "             'ht_dcphase': talib_ht_dcphase,\n",
    "             'ht_phasor_inphase': talib_ht_phasor_inphase,\n",
    "             'ht_phasor_quadrature': talib_ht_phasor_quadrature,\n",
    "             'ht_sine_sine': talib_ht_sine_sine,\n",
    "             'ht_sine_leadsine': talib_ht_sine_leadsine,\n",
    "             'ht_trendmod': talib_ht_trendmode,\n",
    "             # TA-Lib Price Transform Functions\n",
    "             'avgprice': talib_avgprice,\n",
    "             'medprice': talib_medprice,\n",
    "             'typprice': talib_typprice,\n",
    "             'wclprice': talib_wclprice,\n",
    "             }\n",
    "        )\n",
    "\n",
    "        # Need a proper date type\n",
    "        volume_volatility_cycle_price_df['Date'] = pd.to_datetime(\n",
    "            volume_volatility_cycle_price_df['Date'])\n",
    "\n",
    "        return volume_volatility_cycle_price_df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Pattern indicators\n",
    "https://github.com/TA-Lib/ta-lib-python/blob/master/docs/func_groups/pattern_recognition.md"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def talib_get_pattern_recognition_indicators(df: pd.DataFrame) -> pd.DataFrame:\n",
    "# TA-Lib Pattern Recognition indicators\n",
    "        # https://github.com/TA-Lib/ta-lib-python/blob/master/docs/func_groups/pattern_recognition.md\n",
    "        # Nice article about candles (pattern recognition) https://medium.com/analytics-vidhya/recognizing-over-50-candlestick-patterns-with-python-4f02a1822cb5\n",
    "\n",
    "        # CDL2CROWS - Two Crows\n",
    "        talib_cdl2crows = talib.CDL2CROWS(\n",
    "            df.Open.values, df.High.values, df.Low.values, df.Close.values)\n",
    "        # CDL3BLACKCROWS - Three Black Crows\n",
    "        talib_cdl3blackrows = talib.CDL3BLACKCROWS(\n",
    "            df.Open.values, df.High.values, df.Low.values, df.Close.values)\n",
    "        # CDL3INSIDE - Three Inside Up/Down\n",
    "        talib_cdl3inside = talib.CDL3INSIDE(\n",
    "            df.Open.values, df.High.values, df.Low.values, df.Close.values)\n",
    "        # CDL3LINESTRIKE - Three-Line Strike\n",
    "        talib_cdl3linestrike = talib.CDL3LINESTRIKE(\n",
    "            df.Open.values, df.High.values, df.Low.values, df.Close.values)\n",
    "        # CDL3OUTSIDE - Three Outside Up/Down\n",
    "        talib_cdl3outside = talib.CDL3OUTSIDE(\n",
    "            df.Open.values, df.High.values, df.Low.values, df.Close.values)\n",
    "        # CDL3STARSINSOUTH - Three Stars In The South\n",
    "        talib_cdl3starsinsouth = talib.CDL3STARSINSOUTH(\n",
    "            df.Open.values, df.High.values, df.Low.values, df.Close.values)\n",
    "        # CDL3WHITESOLDIERS - Three Advancing White Soldiers\n",
    "        talib_cdl3whitesoldiers = talib.CDL3WHITESOLDIERS(\n",
    "            df.Open.values, df.High.values, df.Low.values, df.Close.values)\n",
    "        # CDLABANDONEDBABY - Abandoned Baby\n",
    "        talib_cdlabandonedbaby = talib.CDLABANDONEDBABY(\n",
    "            df.Open.values, df.High.values, df.Low.values, df.Close.values, penetration=0)\n",
    "        # CDLADVANCEBLOCK - Advance Block\n",
    "        talib_cdladvancedblock = talib.CDLADVANCEBLOCK(\n",
    "            df.Open.values, df.High.values, df.Low.values, df.Close.values)\n",
    "        # CDLBELTHOLD - Belt-hold\n",
    "        talib_cdlbelthold = talib.CDLBELTHOLD(\n",
    "            df.Open.values, df.High.values, df.Low.values, df.Close.values)\n",
    "        # CDLBREAKAWAY - Breakaway\n",
    "        talib_cdlbreakaway = talib.CDLBREAKAWAY(\n",
    "            df.Open.values, df.High.values, df.Low.values, df.Close.values)\n",
    "        # CDLCLOSINGMARUBOZU - Closing Marubozu\n",
    "        talib_cdlclosingmarubozu = talib.CDLCLOSINGMARUBOZU(\n",
    "            df.Open.values, df.High.values, df.Low.values, df.Close.values)\n",
    "        # CDLCONCEALBABYSWALL - Concealing Baby Swallow\n",
    "        talib_cdlconcealbabyswall = talib.CDLCONCEALBABYSWALL(\n",
    "            df.Open.values, df.High.values, df.Low.values, df.Close.values)\n",
    "        # CDLCOUNTERATTACK - Counterattack\n",
    "        talib_cdlcounterattack = talib.CDLCOUNTERATTACK(\n",
    "            df.Open.values, df.High.values, df.Low.values, df.Close.values)\n",
    "        # CDLDARKCLOUDCOVER - Dark Cloud Cover\n",
    "        talib_cdldarkcloudcover = talib.CDLDARKCLOUDCOVER(\n",
    "            df.Open.values, df.High.values, df.Low.values, df.Close.values, penetration=0)\n",
    "        # CDLDOJI - Doji\n",
    "        talib_cdldoji = talib.CDLDOJI(\n",
    "            df.Open.values, df.High.values, df.Low.values, df.Close.values)\n",
    "        # CDLDOJISTAR - Doji Star\n",
    "        talib_cdldojistar = talib.CDLDOJISTAR(\n",
    "            df.Open.values, df.High.values, df.Low.values, df.Close.values)\n",
    "        # CDLDRAGONFLYDOJI - Dragonfly Doji\n",
    "        talib_cdldragonflydoji = talib.CDLDRAGONFLYDOJI(\n",
    "            df.Open.values, df.High.values, df.Low.values, df.Close.values)\n",
    "        # CDLENGULFING - Engulfing Pattern\n",
    "        talib_cdlengulfing = talib.CDLENGULFING(\n",
    "            df.Open.values, df.High.values, df.Low.values, df.Close.values)\n",
    "\n",
    "        # CDLEVENINGDOJISTAR - Evening Doji Star\n",
    "        talib_cdleveningdojistar = talib.CDLEVENINGDOJISTAR(\n",
    "            df.Open.values, df.High.values, df.Low.values, df.Close.values, penetration=0)\n",
    "        # CDLEVENINGSTAR - Evening Star\n",
    "        talib_cdleveningstar = talib.CDLEVENINGSTAR(\n",
    "            df.Open.values, df.High.values, df.Low.values, df.Close.values, penetration=0)\n",
    "        # CDLGAPSIDESIDEWHITE - Up/Down-gap side-by-side white lines\n",
    "        talib_cdlgapsidesidewhite = talib.CDLGAPSIDESIDEWHITE(\n",
    "            df.Open.values, df.High.values, df.Low.values, df.Close.values)\n",
    "        # CDLGRAVESTONEDOJI - Gravestone Doji\n",
    "        talib_cdlgravestonedoji = talib.CDLGRAVESTONEDOJI(\n",
    "            df.Open.values, df.High.values, df.Low.values, df.Close.values)\n",
    "        # CDLHAMMER - Hammer\n",
    "        talib_cdlhammer = talib.CDLHAMMER(\n",
    "            df.Open.values, df.High.values, df.Low.values, df.Close.values)\n",
    "        # CDLHANGINGMAN - Hanging Man\n",
    "        talib_cdlhangingman = talib.CDLHANGINGMAN(\n",
    "            df.Open.values, df.High.values, df.Low.values, df.Close.values)\n",
    "        # CDLHARAMI - Harami Pattern\n",
    "        talib_cdlharami = talib.CDLHARAMI(\n",
    "            df.Open.values, df.High.values, df.Low.values, df.Close.values)\n",
    "        # CDLHARAMICROSS - Harami Cross Pattern\n",
    "        talib_cdlharamicross = talib.CDLHARAMICROSS(\n",
    "            df.Open.values, df.High.values, df.Low.values, df.Close.values)\n",
    "        # CDLHIGHWAVE - High-Wave Candle\n",
    "        talib_cdlhighwave = talib.CDLHIGHWAVE(\n",
    "            df.Open.values, df.High.values, df.Low.values, df.Close.values)\n",
    "        # CDLHIKKAKE - Hikkake Pattern\n",
    "        talib_cdlhikkake = talib.CDLHIKKAKE(\n",
    "            df.Open.values, df.High.values, df.Low.values, df.Close.values)\n",
    "        # CDLHIKKAKEMOD - Modified Hikkake Pattern\n",
    "        talib_cdlhikkakemod = talib.CDLHIKKAKEMOD(\n",
    "            df.Open.values, df.High.values, df.Low.values, df.Close.values)\n",
    "\n",
    "        # CDLHOMINGPIGEON - Homing Pigeon\n",
    "        talib_cdlhomingpigeon = talib.CDLHOMINGPIGEON(\n",
    "            df.Open.values, df.High.values, df.Low.values, df.Close.values)\n",
    "        # CDLIDENTICAL3CROWS - Identical Three Crows\n",
    "        talib_cdlidentical3crows = talib.CDLIDENTICAL3CROWS(\n",
    "            df.Open.values, df.High.values, df.Low.values, df.Close.values)\n",
    "        # CDLINNECK - In-Neck Pattern\n",
    "        talib_cdlinneck = talib.CDLINNECK(\n",
    "            df.Open.values, df.High.values, df.Low.values, df.Close.values)\n",
    "        # CDLINVERTEDHAMMER - Inverted Hammer\n",
    "        talib_cdlinvertedhammer = talib.CDLINVERTEDHAMMER(\n",
    "            df.Open.values, df.High.values, df.Low.values, df.Close.values)\n",
    "        # CDLKICKING - Kicking\n",
    "        talib_cdlkicking = talib.CDLKICKING(\n",
    "            df.Open.values, df.High.values, df.Low.values, df.Close.values)\n",
    "        # CDLKICKINGBYLENGTH - Kicking - bull/bear determined by the longer marubozu\n",
    "        talib_cdlkickingbylength = talib.CDLKICKINGBYLENGTH(\n",
    "            df.Open.values, df.High.values, df.Low.values, df.Close.values)\n",
    "        # CDLLADDERBOTTOM - Ladder Bottom\n",
    "        talib_cdlladderbottom = talib.CDLLADDERBOTTOM(\n",
    "            df.Open.values, df.High.values, df.Low.values, df.Close.values)\n",
    "        # CDLLONGLEGGEDDOJI - Long Legged Doji\n",
    "        talib_cdllongleggeddoji = talib.CDLLONGLEGGEDDOJI(\n",
    "            df.Open.values, df.High.values, df.Low.values, df.Close.values)\n",
    "        # CDLLONGLINE - Long Line Candle\n",
    "        talib_cdllongline = talib.CDLLONGLINE(\n",
    "            df.Open.values, df.High.values, df.Low.values, df.Close.values)\n",
    "        # CDLMARUBOZU - Marubozu\n",
    "        talib_cdlmarubozu = talib.CDLMARUBOZU(\n",
    "            df.Open.values, df.High.values, df.Low.values, df.Close.values)\n",
    "        # CDLMATCHINGLOW - Matching Low\n",
    "        talib_cdlmatchinglow = talib.CDLMATCHINGLOW(\n",
    "            df.Open.values, df.High.values, df.Low.values, df.Close.values)\n",
    "\n",
    "        # CDLMATHOLD - Mat Hold\n",
    "        talib_cdlmathold = talib.CDLMATHOLD(\n",
    "            df.Open.values, df.High.values, df.Low.values, df.Close.values, penetration=0)\n",
    "        # CDLMORNINGDOJISTAR - Morning Doji Star\n",
    "        talib_cdlmorningdojistar = talib.CDLMORNINGDOJISTAR(\n",
    "            df.Open.values, df.High.values, df.Low.values, df.Close.values, penetration=0)\n",
    "        # CDLMORNINGSTAR - Morning Star\n",
    "        talib_cdlmorningstar = talib.CDLMORNINGSTAR(\n",
    "            df.Open.values, df.High.values, df.Low.values, df.Close.values, penetration=0)\n",
    "        # CDLONNECK - On-Neck Pattern\n",
    "        talib_cdlonneck = talib.CDLONNECK(\n",
    "            df.Open.values, df.High.values, df.Low.values, df.Close.values)\n",
    "        # CDLPIERCING - Piercing Pattern\n",
    "        talib_cdlpiercing = talib.CDLPIERCING(\n",
    "            df.Open.values, df.High.values, df.Low.values, df.Close.values)\n",
    "        # CDLRICKSHAWMAN - Rickshaw Man\n",
    "        talib_cdlrickshawman = talib.CDLRICKSHAWMAN(\n",
    "            df.Open.values, df.High.values, df.Low.values, df.Close.values)\n",
    "        # CDLRISEFALL3METHODS - Rising/Falling Three Methods\n",
    "        talib_cdlrisefall3methods = talib.CDLRISEFALL3METHODS(\n",
    "            df.Open.values, df.High.values, df.Low.values, df.Close.values)\n",
    "        # CDLSEPARATINGLINES - Separating Lines\n",
    "        talib_cdlseparatinglines = talib.CDLSEPARATINGLINES(\n",
    "            df.Open.values, df.High.values, df.Low.values, df.Close.values)\n",
    "        # CDLSHOOTINGSTAR - Shooting Star\n",
    "        talib_cdlshootingstar = talib.CDLSHOOTINGSTAR(\n",
    "            df.Open.values, df.High.values, df.Low.values, df.Close.values)\n",
    "        # CDLSHORTLINE - Short Line Candle\n",
    "        talib_cdlshortline = talib.CDLSHORTLINE(\n",
    "            df.Open.values, df.High.values, df.Low.values, df.Close.values)\n",
    "        # CDLSPINNINGTOP - Spinning Top\n",
    "        talib_cdlspinningtop = talib.CDLSPINNINGTOP(\n",
    "            df.Open.values, df.High.values, df.Low.values, df.Close.values)\n",
    "\n",
    "        # CDLSTALLEDPATTERN - Stalled Pattern\n",
    "        talib_cdlstalledpattern = talib.CDLSTALLEDPATTERN(\n",
    "            df.Open.values, df.High.values, df.Low.values, df.Close.values)\n",
    "        # CDLSTICKSANDWICH - Stick Sandwich\n",
    "        talib_cdlsticksandwich = talib.CDLSTICKSANDWICH(\n",
    "            df.Open.values, df.High.values, df.Low.values, df.Close.values)\n",
    "        # CDLTAKURI - Takuri (Dragonfly Doji with very long lower shadow)\n",
    "        talib_cdltakuru = talib.CDLTAKURI(\n",
    "            df.Open.values, df.High.values, df.Low.values, df.Close.values)\n",
    "        # CDLTASUKIGAP - Tasuki Gap\n",
    "        talib_cdltasukigap = talib.CDLTASUKIGAP(\n",
    "            df.Open.values, df.High.values, df.Low.values, df.Close.values)\n",
    "        # CDLTHRUSTING - Thrusting Pattern\n",
    "        talib_cdlthrusting = talib.CDLTHRUSTING(\n",
    "            df.Open.values, df.High.values, df.Low.values, df.Close.values)\n",
    "        # CDLTRISTAR - Tristar Pattern\n",
    "        talib_cdltristar = talib.CDLTRISTAR(\n",
    "            df.Open.values, df.High.values, df.Low.values, df.Close.values)\n",
    "        # CDLUNIQUE3RIVER - Unique 3 River\n",
    "        talib_cdlunique3river = talib.CDLUNIQUE3RIVER(\n",
    "            df.Open.values, df.High.values, df.Low.values, df.Close.values)\n",
    "        # CDLUPSIDEGAP2CROWS - Upside Gap Two Crows\n",
    "        talib_cdlupsidegap2crows = talib.CDLUPSIDEGAP2CROWS(\n",
    "            df.Open.values, df.High.values, df.Low.values, df.Close.values)\n",
    "        # CDLXSIDEGAP3METHODS - Upside/Downside Gap Three Methods\n",
    "        talib_cdlxsidegap3methods = talib.CDLXSIDEGAP3METHODS(\n",
    "            df.Open.values, df.High.values, df.Low.values, df.Close.values)\n",
    "\n",
    "        pattern_indicators_df = pd.DataFrame(\n",
    "            {'Date': df.Date.values,\n",
    "             'Ticker': df.Ticker,\n",
    "             # TA-Lib Pattern Recognition indicators\n",
    "             'cdl2crows': talib_cdl2crows,\n",
    "             'cdl3blackrows': talib_cdl3blackrows,\n",
    "             'cdl3inside': talib_cdl3inside,\n",
    "             'cdl3linestrike': talib_cdl3linestrike,\n",
    "             'cdl3outside': talib_cdl3outside,\n",
    "             'cdl3starsinsouth': talib_cdl3starsinsouth,\n",
    "             'cdl3whitesoldiers': talib_cdl3whitesoldiers,\n",
    "             'cdlabandonedbaby': talib_cdlabandonedbaby,\n",
    "             'cdladvancedblock': talib_cdladvancedblock,\n",
    "             'cdlbelthold': talib_cdlbelthold,\n",
    "             'cdlbreakaway': talib_cdlbreakaway,\n",
    "             'cdlclosingmarubozu': talib_cdlclosingmarubozu,\n",
    "             'cdlconcealbabyswall': talib_cdlconcealbabyswall,\n",
    "             'cdlcounterattack': talib_cdlcounterattack,\n",
    "             'cdldarkcloudcover': talib_cdldarkcloudcover,\n",
    "             'cdldoji': talib_cdldoji,\n",
    "             'cdldojistar': talib_cdldojistar,\n",
    "             'cdldragonflydoji': talib_cdldragonflydoji,\n",
    "             'cdlengulfing': talib_cdlengulfing,\n",
    "             'cdleveningdojistar': talib_cdleveningdojistar,\n",
    "             'cdleveningstar': talib_cdleveningstar,\n",
    "             'cdlgapsidesidewhite': talib_cdlgapsidesidewhite,\n",
    "             'cdlgravestonedoji': talib_cdlgravestonedoji,\n",
    "             'cdlhammer': talib_cdlhammer,\n",
    "             'cdlhangingman': talib_cdlhangingman,\n",
    "             'cdlharami': talib_cdlharami,\n",
    "             'cdlharamicross': talib_cdlharamicross,\n",
    "             'cdlhighwave': talib_cdlhighwave,\n",
    "             'cdlhikkake': talib_cdlhikkake,\n",
    "             'cdlhikkakemod': talib_cdlhikkakemod,\n",
    "             'cdlhomingpigeon': talib_cdlhomingpigeon,\n",
    "             'cdlidentical3crows': talib_cdlidentical3crows,\n",
    "             'cdlinneck': talib_cdlinneck,\n",
    "             'cdlinvertedhammer': talib_cdlinvertedhammer,\n",
    "             'cdlkicking': talib_cdlkicking,\n",
    "             'cdlkickingbylength': talib_cdlkickingbylength,\n",
    "             'cdlladderbottom': talib_cdlladderbottom,\n",
    "             'cdllongleggeddoji': talib_cdllongleggeddoji,\n",
    "             'cdllongline': talib_cdllongline,\n",
    "             'cdlmarubozu': talib_cdlmarubozu,\n",
    "             'cdlmatchinglow': talib_cdlmatchinglow,\n",
    "             'cdlmathold': talib_cdlmathold,\n",
    "             'cdlmorningdojistar': talib_cdlmorningdojistar,\n",
    "             'cdlmorningstar': talib_cdlmorningstar,\n",
    "             'cdlonneck': talib_cdlonneck,\n",
    "             'cdlpiercing': talib_cdlpiercing,\n",
    "             'cdlrickshawman': talib_cdlrickshawman,\n",
    "             'cdlrisefall3methods': talib_cdlrisefall3methods,\n",
    "             'cdlseparatinglines': talib_cdlseparatinglines,\n",
    "             'cdlshootingstar': talib_cdlshootingstar,\n",
    "             'cdlshortline': talib_cdlshortline,\n",
    "             'cdlspinningtop': talib_cdlspinningtop,\n",
    "             'cdlstalledpattern': talib_cdlstalledpattern,\n",
    "             'cdlsticksandwich': talib_cdlsticksandwich,\n",
    "             'cdltakuru': talib_cdltakuru,\n",
    "             'cdltasukigap': talib_cdltasukigap,\n",
    "             'cdlthrusting': talib_cdlthrusting,\n",
    "             'cdltristar': talib_cdltristar,\n",
    "             'cdlunique3river': talib_cdlunique3river,\n",
    "             'cdlupsidegap2crows': talib_cdlupsidegap2crows,\n",
    "             'cdlxsidegap3methods': talib_cdlxsidegap3methods\n",
    "             }\n",
    "        )\n",
    "\n",
    "        # Need a proper date type\n",
    "        pattern_indicators_df['Date'] = pd.to_datetime(\n",
    "            pattern_indicators_df['Date'])\n",
    "\n",
    "        return pattern_indicators_df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Calculate Tech Indicators and Merge to the original dataframe"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Volume needs to be float, not int\n",
    "stocks_df['Volume'] = stocks_df['Volume']*1.0"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# to resolve an error \"Exception: input array type is not double\"\n",
    "# https://stackoverflow.com/questions/51712269/how-to-run-ta-lib-on-multiple-columns-of-a-pandas-dataframe\n",
    "for f in ['Open','High','Low','Close', 'Volume', 'Adj Close']:\n",
    "  stocks_df.loc[:,f] = stocks_df.loc[:,f].astype('float64')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#stocks_df.info()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# supress warnings\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "\n",
    "# adding Momentum / Pattern/ Volume features to all tickers - one by one\n",
    "merged_df_with_tech_ind = pd.DataFrame({'A' : []})\n",
    "\n",
    "current_ticker_data = None\n",
    "i=0\n",
    "for ticker in ALL_TICKERS:\n",
    "  i+=1\n",
    "  print(f'{i}/{len(ALL_TICKERS)} Current ticker is {ticker}')\n",
    "  current_ticker_data = stocks_df[stocks_df.Ticker.isin([ticker])]\n",
    "  # need to have same 'utc' time on both sides\n",
    "  # https://stackoverflow.com/questions/73964894/you-are-trying-to-merge-on-datetime64ns-utc-and-datetime64ns-columns-if-yo\n",
    "  current_ticker_data['Date']= pd.to_datetime(current_ticker_data['Date'], utc=True)\n",
    "\n",
    "  # 3 calls to get additional features\n",
    "  df_current_ticker_momentum_indicators = talib_get_momentum_indicators_for_one_ticker(current_ticker_data)\n",
    "  df_current_ticker_momentum_indicators[\"Date\"]= pd.to_datetime(df_current_ticker_momentum_indicators['Date'], utc=True)\n",
    "  # df_current_ticker_momentum_indicators.loc[:,\"Date\"]= pd.to_datetime(df_current_ticker_momentum_indicators['Date'], utc=True)\n",
    "\n",
    "  df_current_ticker_volume_indicators = talib_get_volume_volatility_cycle_price_indicators(current_ticker_data)\n",
    "  df_current_ticker_volume_indicators[\"Date\"]= pd.to_datetime(df_current_ticker_volume_indicators['Date'], utc=True)\n",
    "  # df_current_ticker_volume_indicators.loc[:,\"Date\"]= pd.to_datetime(df_current_ticker_volume_indicators['Date'], utc=True)\n",
    "\n",
    "  df_current_ticker_pattern_indicators = talib_get_pattern_recognition_indicators(current_ticker_data)\n",
    "  df_current_ticker_pattern_indicators[\"Date\"]= pd.to_datetime(df_current_ticker_pattern_indicators['Date'], utc=True)\n",
    "  # df_current_ticker_pattern_indicators.loc[:,\"Date\"]= pd.to_datetime(df_current_ticker_pattern_indicators['Date'], utc=True)\n",
    "\n",
    "  # merge to one df\n",
    "  m1 = pd.merge(current_ticker_data, df_current_ticker_momentum_indicators.reset_index(), how = 'left', on = [\"Date\",\"Ticker\"], validate = \"one_to_one\")\n",
    "  m2 = pd.merge(m1, df_current_ticker_volume_indicators.reset_index(), how = 'left', on = [\"Date\",\"Ticker\"], validate = \"one_to_one\")\n",
    "  m3 = pd.merge(m2, df_current_ticker_pattern_indicators.reset_index(), how = 'left', on = [\"Date\",\"Ticker\"], validate = \"one_to_one\")\n",
    "\n",
    "  if merged_df_with_tech_ind.empty:\n",
    "    merged_df_with_tech_ind = m3\n",
    "  else:\n",
    "    merged_df_with_tech_ind = pd.concat([merged_df_with_tech_ind,m3], ignore_index = False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#merged_df_with_tech_ind[merged_df_with_tech_ind.Date=='2024-04-01'].tail()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#merged_df_with_tech_ind.info()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#merged_df_with_tech_ind.count()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Merge Data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#merged_df_with_tech_ind.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# make sure it is dateTime with no timezone\n",
    "merged_df_with_tech_ind['Date'] = pd.to_datetime(merged_df_with_tech_ind['Date']).dt.tz_localize(None)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "m2 = pd.merge(merged_df_with_tech_ind,\n",
    "              df_dax_to_merge,\n",
    "              how='left',\n",
    "              left_on='Date',\n",
    "              right_index=True,\n",
    "              validate = \"many_to_one\"\n",
    "              )"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#m2.tail(1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# check the same number of records, but columns increased\n",
    "#m2.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# merge with dax_daily_to_merge\n",
    "#snp500_to_merge.tail(1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "m3 = pd.merge(m2,\n",
    "              snp500_to_merge,\n",
    "              how='left',\n",
    "              left_on='Date',\n",
    "              right_index=True,\n",
    "              validate = \"many_to_one\"\n",
    "              )"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#m3.tail(1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#m3.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "m4 = pd.merge(m3,\n",
    "              dji_daily_to_merge,\n",
    "              how='left',\n",
    "              left_on='Date',\n",
    "              right_index=True,\n",
    "              validate = \"many_to_one\"\n",
    "              )"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#m4.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#m4.tail(1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# define quarter as the first date of qtr\n",
    "m4['Quarter'] = m4['Date'].dt.to_period('Q').dt.to_timestamp()\n",
    "#m4['Quarter']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#gdp_to_merge.tail(1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "m5 = pd.merge(m4,\n",
    "              gdp_to_merge,\n",
    "              how='left',\n",
    "              left_on='Quarter',\n",
    "              right_index=True,\n",
    "              validate = \"many_to_one\"\n",
    "              )"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#m5.tail(2)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#m5.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "m5['Month'] = m5['Date'].dt.to_period('M').dt.to_timestamp()\n",
    "#m5['Month']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#cpi_to_merge.tail(1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "m6 = pd.merge(m5,\n",
    "              cpi_to_merge,\n",
    "              how='left',\n",
    "              left_on='Month',\n",
    "              right_index=True,\n",
    "              validate = \"many_to_one\"\n",
    "              )"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#m6.tail(1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#bond10.tail(1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "m7 = pd.merge(m6,\n",
    "              bond10,\n",
    "              how='left',\n",
    "              left_on='Month',\n",
    "              right_index=True,\n",
    "              validate = \"many_to_one\"\n",
    "              )"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#m7.tail(1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fields_to_fill = ['cpi_de_yoy',\t'cpi_de_mom', \"gdp_de_yoy\", \"gdp_de_qoq\", \"bond10\"]\n",
    "# Fill missing values in selected fields with the last defined value\n",
    "for field in fields_to_fill:\n",
    "    m7[field] = m7[field].ffill()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#m7.tail(1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#gold_to_merge.tail(2)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "m8 = pd.merge(m7,\n",
    "              gold_to_merge,\n",
    "              how='left',\n",
    "              left_on='Date',\n",
    "              right_index=True,\n",
    "              validate = \"many_to_one\"\n",
    "              )"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#m8.tail(2)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "m9 = pd.merge(m8,\n",
    "              crude_oil_to_merge,\n",
    "              how='left',\n",
    "              left_on='Date',\n",
    "              right_index=True,\n",
    "              validate = \"many_to_one\"\n",
    "              )"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#m9.tail(2)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "m10 = pd.merge(m9,\n",
    "              brent_oil_to_merge,\n",
    "              how='left',\n",
    "              left_on='Date',\n",
    "              right_index=True,\n",
    "              validate = \"many_to_one\"\n",
    "              )"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#m10.tail(2)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "m11 = pd.merge(m10,\n",
    "              btc_usd_to_merge,\n",
    "              how='left',\n",
    "              left_on='Date',\n",
    "              right_index=True,\n",
    "              validate = \"many_to_one\"\n",
    "              )"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#m11.tail(2)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#vix_to_merge.tail(2)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "m12 = pd.merge(m11,\n",
    "              vix_to_merge,\n",
    "              how='left',\n",
    "              left_on='Date',\n",
    "              right_index=True,\n",
    "              validate = \"many_to_one\"\n",
    "              )\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#m12.tail(2)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#m11.info()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#m11.count()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#date = m11.Date.max()\n",
    "#date_str = date.strftime('%Y_%m_%d')\n",
    "#print(date_str)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data Prep"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# full dataset for 33 stocks\n",
    "df_full = m11.copy()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Feature sets"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# growth indicators (but not future growth)\n",
    "GROWTH = [g for g in df_full.keys() if (g.find('growth_')==0)&(g.find('future')<0)]\n",
    "#GROWTH"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# leaving only Volume ==> generate ln(Volume)\n",
    "OHLCV = ['Open','High','Low','Close','Adj Close','Volume']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "CATEGORICAL = ['Month', 'Weekday', 'Ticker', 'ticker_type']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "TO_PREDICT = [g for g in df_full.keys() if (g.find('future')>=0)]\n",
    "#TO_PREDICT"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "TO_DROP = ['Year','Date','index_x', 'index_y', 'index', 'Quarter'] + CATEGORICAL + OHLCV\n",
    "#TO_DROP , 'vix_adj_close'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# let's define on more custom numerical features\n",
    "df_full['ln_volume'] = df_full.Volume.apply(lambda x: np.log(x))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# manually defined features\n",
    "CUSTOM_NUMERICAL = ['SMA10', 'SMA20', 'growing_moving_average', 'high_minus_low_relative','volatility', 'ln_volume']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# All Supported Ta-lib indicators: https://github.com/TA-Lib/ta-lib-python/blob/master/docs/funcs.md\n",
    "\n",
    "TECHNICAL_INDICATORS = ['adx', 'adxr', 'apo', 'aroon_1','aroon_2', 'aroonosc',\n",
    " 'bop', 'cci', 'cmo','dx', 'macd', 'macdsignal', 'macdhist', 'macd_ext',\n",
    " 'macdsignal_ext', 'macdhist_ext', 'macd_fix', 'macdsignal_fix',\n",
    " 'macdhist_fix', 'mfi', 'minus_di', 'mom', 'plus_di', 'dm', 'ppo',\n",
    " 'roc', 'rocp', 'rocr', 'rocr100', 'rsi', 'slowk', 'slowd', 'fastk',\n",
    " 'fastd', 'fastk_rsi', 'fastd_rsi', 'trix', 'ultosc', 'willr',\n",
    " 'ad', 'adosc', 'obv', 'atr', 'natr', 'ht_dcperiod', 'ht_dcphase',\n",
    " 'ht_phasor_inphase', 'ht_phasor_quadrature', 'ht_sine_sine', 'ht_sine_leadsine',\n",
    " 'ht_trendmod', 'avgprice', 'medprice', 'typprice', 'wclprice']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "TECHNICAL_PATTERNS = [g for g in df_full.keys() if g.find('cdl')>=0]\n",
    "#print(f'Technical patterns count = {len(TECHNICAL_PATTERNS)}, examples = {TECHNICAL_PATTERNS[0:5]}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "MACRO = ['gdp_de_yoy', 'gdp_de_qoq', 'cpi_de_yoy', 'cpi_de_mom', 'bond10']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "NUMERICAL = GROWTH + TECHNICAL_INDICATORS + TECHNICAL_PATTERNS + CUSTOM_NUMERICAL + MACRO"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# CHECK: NO OTHER INDICATORS LEFT\n",
    "OTHER = [k for k in df_full.keys() if k not in OHLCV + CATEGORICAL + NUMERICAL + TO_DROP + TO_PREDICT]\n",
    "#OTHER"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#df_full.Ticker.nunique()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# truncated df_full with 25 years of data (and defined growth variables)\n",
    "df = df_full[df_full.Date>='2000-01-01']\n",
    "#df.info()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Dummies"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# dummy variables can't be generated from Date and numeric variables ==> convert to STRING (to define groups for Dummies)\n",
    "df.loc[:,'Month'] = df.Month.dt.strftime('%B')\n",
    "df.loc[:,'Weekday'] = df.Weekday.astype(str)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# define week of month\n",
    "df.loc[:,'wom'] = df.Date.apply(lambda d: (d.day-1)//7 + 1)\n",
    "# convert to string\n",
    "df.loc[:,'wom'] = df.loc[:,'wom'].astype(str)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# check values for week-of-month (should be between 1 and 5)\n",
    "#df.wom.value_counts()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df.loc[:,'month_wom'] = df.Month + '_w' + df.wom"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# examples of encoding\n",
    "#df.month_wom.value_counts()[0:2]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# del wom temp variable\n",
    "del df['wom']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Generate all dummies"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# what are the categorical features?\n",
    "CATEGORICAL.append('month_wom')\n",
    "#CATEGORICAL"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Generate dummy variables (no need for bool, let's have int32 instead)\n",
    "dummy_variables = pd.get_dummies(df[CATEGORICAL], dtype='int32')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#dummy_variables.info()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# get dummies names in a list\n",
    "DUMMIES = dummy_variables.keys().to_list()\n",
    "#DUMMIES"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#len(DUMMIES)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Concatenate the dummy variables with the original DataFrame\n",
    "df_with_dummies = pd.concat([df, dummy_variables], axis=1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#df_with_dummies[NUMERICAL+DUMMIES].info()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "DUMMIES_SHORT = [value for value in DUMMIES if not (value.startswith('month_') or value.startswith('Ticker_'))]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# now check the size of the dataset\n",
    "#df_with_dummies[NUMERICAL+DUMMIES_SHORT].info()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#df_with_dummies[NUMERICAL+DUMMIES_SHORT].count()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Splitting the data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def temporal_split(df, min_date, max_date, train_prop=0.8, val_prop=0.1, test_prop=0.1):\n",
    "    \"\"\"\n",
    "    Splits a DataFrame into three buckets based on the temporal order of the 'Date' column.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): The DataFrame to split.\n",
    "        min_date (str or Timestamp): Minimum date in the DataFrame.\n",
    "        max_date (str or Timestamp): Maximum date in the DataFrame.\n",
    "        train_prop (float): Proportion of data for training set (default: 0.8).\n",
    "        val_prop (float): Proportion of data for validation set (default: 0.1).\n",
    "        test_prop (float): Proportion of data for test set (default: 0.1).\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: The input DataFrame with a new column 'split' indicating the split for each row.\n",
    "    \"\"\"\n",
    "    # Define the date intervals\n",
    "    train_end = min_date + pd.Timedelta(days=(max_date - min_date).days * train_prop)\n",
    "    val_end = train_end + pd.Timedelta(days=(max_date - min_date).days * val_prop)\n",
    "\n",
    "    # Assign split labels based on date ranges\n",
    "    split_labels = []\n",
    "    for date in df['Date']:\n",
    "        if date <= train_end:\n",
    "            split_labels.append('train')\n",
    "        elif date <= val_end:\n",
    "            split_labels.append('validation')\n",
    "        else:\n",
    "            split_labels.append('test')\n",
    "\n",
    "    # Add 'split' column to the DataFrame\n",
    "    df['split'] = split_labels\n",
    "\n",
    "    return df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "min_date_df = df_with_dummies.Date.min()\n",
    "max_date_df = df_with_dummies.Date.max()\n",
    "\n",
    "df_with_dummies = temporal_split(df_with_dummies,\n",
    "                                 min_date = min_date_df,\n",
    "                                 max_date = max_date_df)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# the split is approximate!\n",
    "  # as tickers can have a different history\n",
    "df_with_dummies['split'].value_counts()/len(df_with_dummies)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# remove the \"segmentation\" problem (warning message on df performance after many joins and data transformations)\n",
    "new_df = df_with_dummies.copy()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# EXACT DATES for the split:\n",
    "# time split on train/validation/test: FIXED dates of split, approx. 70%, 15%, 15% split\n",
    "#new_df.groupby(['split'])['Date'].agg({'min','max','count'})"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Full dataframe (transformed and truncated to 25 years)\n",
    "#new_df.info()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# check one record: it has abs. values, text, and numbers\n",
    "#new_df.head(1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# what do we try to predict\n",
    "new_df[TO_PREDICT].head(1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Define & clean dataframes for ML"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Features to be used in predictions (incl. new dummies)\n",
    "\n",
    "# TODO: use DUMMIES_SHORT instead of DUMMIES\n",
    "\n",
    "features_list = NUMERICAL+DUMMIES\n",
    "\n",
    "\n",
    "# What we're trying to predict?\n",
    "\n",
    "# TODO: use another feature to predict 'is_strong_positive_growth_3d_future'\n",
    "\n",
    "to_predict = 'is_positive_growth_3d_future'\n",
    "\n",
    "train_df = new_df[new_df.split.isin(['train'])].copy(deep=True)\n",
    "valid_df = new_df[new_df.split.isin(['validation'])].copy(deep=True)\n",
    "train_valid_df = new_df[new_df.split.isin(['train','validation'])].copy(deep=True)\n",
    "\n",
    "test_df =  new_df[new_df.split.isin(['test'])].copy(deep=True)\n",
    "\n",
    "# ONLY numerical Separate features and target variable for training and testing sets\n",
    "X_train = train_df[features_list+[to_predict]]\n",
    "X_valid = valid_df[features_list+[to_predict]]\n",
    "\n",
    "X_train_valid = train_valid_df[features_list+[to_predict]]\n",
    "\n",
    "X_test = test_df[features_list+[to_predict]]\n",
    "\n",
    "# this to be used for predictions and join to the original dataframe new_df\n",
    "X_all =  new_df[features_list+[to_predict]].copy(deep=True)\n",
    "\n",
    "print(f'length: X_train {X_train.shape},  X_validation {X_valid.shape}, X_test {X_test.shape}, X_train_valid = {X_train_valid.shape},  all combined: X_all {X_all.shape}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Prepare a dataframe for ML:\n",
    "  # +-inf to NaN, all NaNs to 0s\n",
    "def clean_dataframe_from_inf_and_nan(df:pd.DataFrame):\n",
    "  df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "  df.fillna(0, inplace=True)\n",
    "  return df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Clean from +-inf and NaNs:\n",
    "\n",
    "X_train = clean_dataframe_from_inf_and_nan(X_train)\n",
    "X_valid = clean_dataframe_from_inf_and_nan(X_valid)\n",
    "X_train_valid = clean_dataframe_from_inf_and_nan(X_train_valid)\n",
    "X_test = clean_dataframe_from_inf_and_nan(X_test)\n",
    "X_all = clean_dataframe_from_inf_and_nan(X_all)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "y_train = X_train[to_predict]\n",
    "\n",
    "y_valid = X_valid[to_predict]\n",
    "\n",
    "y_train_valid = X_train_valid[to_predict]\n",
    "y_test = X_test[to_predict]\n",
    "y_all =  X_all[to_predict]\n",
    "\n",
    "# remove y_train, y_test from X_ dataframes\n",
    "del X_train[to_predict]\n",
    "del X_valid[to_predict]\n",
    "del X_train_valid[to_predict]\n",
    "\n",
    "del X_test[to_predict]\n",
    "\n",
    "del X_all[to_predict]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Modeling"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Decision Tree Classifier"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Function to find all predictions (starting from 'pred'), generate is_correct (correctness of each prediction)\n",
    "# and precision on TEST dataset (assuming there is df[\"split\"] column with values 'train','validation','test'\n",
    "\n",
    "# returns 2 lists of features: PREDICTIONS and IS_CORRECT\n",
    "\n",
    "def get_predictions_correctness(df:pd.DataFrame, to_predict:str):\n",
    "  PREDICTIONS = [k for k in df.keys() if k.startswith('pred')]\n",
    "  print(f'Prediction columns founded: {PREDICTIONS}')\n",
    "\n",
    "  # add columns is_correct_\n",
    "  for pred in PREDICTIONS:\n",
    "    part1 = pred.split('_')[0] # first prefix before '_'\n",
    "    df[f'is_correct_{part1}'] =  (new_df[pred] == new_df[to_predict]).astype(int)\n",
    "\n",
    "  # IS_CORRECT features set\n",
    "  IS_CORRECT =  [k for k in df.keys() if k.startswith('is_correct_')]\n",
    "  print(f'Created columns is_correct: {IS_CORRECT}')\n",
    "\n",
    "  print('Precision on TEST set for each prediction:')\n",
    "  # define \"Precision\" for ALL predictions on a Test dataset (~4 last years of trading)\n",
    "  for i,column in enumerate(IS_CORRECT):\n",
    "    prediction_column = PREDICTIONS[i]\n",
    "    is_correct_column = column\n",
    "    filter = (new_df.split=='test') & (new_df[prediction_column]==1)\n",
    "    print(f'Prediction column:{prediction_column} , is_correct_column: {is_correct_column}')\n",
    "    print(new_df[filter][is_correct_column].value_counts())\n",
    "    print(new_df[filter][is_correct_column].value_counts()/len(new_df[filter]))\n",
    "    print('---------')\n",
    "\n",
    "  return PREDICTIONS, IS_CORRECT"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Estimation/fit function (using dataframe of features X and what to predict y) --> optimising total accuracy\n",
    "# max_depth is hyperParameter\n",
    "def fit_decision_tree(X, y, max_depth=20):\n",
    "# Initialize the Decision Tree Classifier\n",
    "  clf = DecisionTreeClassifier(max_depth=max_depth,\n",
    "                               random_state=42)\n",
    "\n",
    "  # Fit the classifier to the training data\n",
    "  clf.fit(X, y)\n",
    "  return clf, X.columns"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%time\n",
    "clf_15, train_columns = fit_decision_tree(X=X_train_valid,\n",
    "                           y=y_train_valid,\n",
    "                           max_depth=15)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# predict on a full dataset\n",
    "y_pred_all = clf_15.predict(X_all)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# defining a new prediction vector is easy now, as the dimensions will match\n",
    "new_df['pred1_clf_15'] = y_pred_all"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# new prediction is added --> need to recalculate the correctness\n",
    "#PREDICTIONS, IS_CORRECT = get_predictions_correctness(df = new_df, to_predict = to_predict)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# For a DecisionTreeClassifier in scikit-learn, the concept of trainable parameters differs from that of neural networks.\n",
    "# In decision trees, the parameters are the structure of the tree itself (nodes and splits) rather than weights.\n",
    "# However, you can still get a sense of the model's complexity by looking at the number of nodes and leaves.\n",
    "\n",
    "# Here's how you can get this information for your trained DecisionTreeClassifier (referred to as clf_best):\n",
    "\n",
    "# Get the number of nodes and leaves in the tree\n",
    "n_nodes = clf_15.tree_.node_count\n",
    "n_leaves = clf_15.get_n_leaves()\n",
    "\n",
    "print(f\"Number of nodes: {n_nodes}\")\n",
    "print(f\"Number of leaves: {n_leaves}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Random Forest"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Suboptimal Predictor with less (than optimal) Estimators (200) and lower Max_Depth (17)\n",
    "# several minutes to compute (6 min)\n",
    "\n",
    "rf_best = RandomForestClassifier(n_estimators = 200,\n",
    "                                 max_depth = 17,\n",
    "                                 random_state = 42,\n",
    "                                 n_jobs = -1)\n",
    "\n",
    "rf_best = rf_best.fit(X_train_valid, y_train_valid)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# predict on a full dataset\n",
    "y_pred_all = rf_best.predict(X_all)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# defining a new prediction vector is easy now, as the dimensions will match\n",
    "new_df['pred2_rf_best'] = y_pred_all"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# new prediction is added --> need to recalculate the correctness\n",
    "PREDICTIONS, IS_CORRECT = get_predictions_correctness(df = new_df, to_predict = to_predict)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Trading Simulations"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Total Number of Days (~4 years of trading)\n",
    "PREDICTIONS"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# check actual future growth with 'growth_future_5d', correctness of the prediction with 'is_positive_growth_5d_future'\n",
    "TO_PREDICT"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Calculate fin. result for ALL predictions (manual and produced by models)\n",
    "# Strategy is 50$ investment for each positive prediction (1) for 3 days ahead\n",
    "\n",
    "investment = 50 # variable for the investement value\n",
    "yTrad = round(new_df[new_df.split=='test'].Date.nunique()/252,1) # variable for the number of years of trading\n",
    "\n",
    "sim1_results = []  # results in Array\n",
    "\n",
    "# Iterate over all predictions\n",
    "for pred in PREDICTIONS:\n",
    "    print(f'Calculating sumulation for prediction {pred}:')\n",
    "    print(\n",
    "        f\"    Count times of investment {len(new_df[(new_df.split == 'test') & (new_df[pred] == 1)])} out of {len(new_df[(new_df.split == 'test')])} TEST records\")\n",
    "\n",
    "    # Prefix: e.g. pred1 or pred10\n",
    "    pred_prefix = pred.split('_')[0]\n",
    "\n",
    "    # Fin. result columns: define new records for EACH positive prediction\n",
    "    new_df['sim1_gross_rev_' + pred_prefix] = new_df[pred] * investment * (new_df['growth_future_3d'] - 1)\n",
    "    new_df['sim1_fees_' + pred_prefix] = -new_df[pred] * investment * 0.002\n",
    "    new_df['sim1_net_rev_' + pred_prefix] = new_df['sim1_gross_rev_' + pred_prefix] + new_df['sim1_fees_' + pred_prefix]\n",
    "\n",
    "    # calculate agg. results for each PREDICTION columns (pred) on TEST\n",
    "    filter_test_and_positive_pred = (new_df.split == 'test') & (new_df[pred] == 1)  # filter records on TEST set, when current prediction is 1 (we invest $50 for 3 days ahead - 3 periods)\n",
    "    sim1_count_investments = len(new_df[filter_test_and_positive_pred])\n",
    "    sim1_gross_rev = new_df[filter_test_and_positive_pred]['sim1_gross_rev_' + pred_prefix].sum()\n",
    "    sim1_fees = new_df[filter_test_and_positive_pred]['sim1_fees_' + pred_prefix].sum()\n",
    "    sim1_net_rev = new_df[filter_test_and_positive_pred]['sim1_net_rev_' + pred_prefix].sum()\n",
    "\n",
    "    if sim1_gross_rev > 0:\n",
    "        sim1_fees_percentage = -sim1_fees / sim1_gross_rev\n",
    "    else:\n",
    "        sim1_fees_percentage = None\n",
    "\n",
    "    if sim1_count_investments > 0:\n",
    "        sim1_average_net_revenue = sim1_net_rev / sim1_count_investments\n",
    "    else:\n",
    "        sim1_average_net_revenue = None\n",
    "\n",
    "    # APPROXIMATE CAPITAL REQUIRED and CAGR Calculation\n",
    "    df_investments_count_daily = pd.DataFrame(new_df[filter_test_and_positive_pred].groupby('Date')[pred].count())\n",
    "    sim1_avg_investments_per_day = df_investments_count_daily[pred].mean()\n",
    "    sim1_q75_investments_per_day = df_investments_count_daily[pred].quantile(0.75)  # 75% case - how many $50 investments per day do we have?\n",
    "    # df_investments_count_daily[pred].mean()\n",
    "    sim1_capital = investment * 3 * sim1_q75_investments_per_day  # 3 days in a row with positive predictions\n",
    "    # CAGR: average growth per year. E.g. if you have 1.5 return (50% growth in 4 years) --> (1.5)**(1/4) = 1.106 or 10.6% average\n",
    "    sim1_CAGR = ((sim1_capital + sim1_net_rev) / sim1_capital) ** (1 / yTrad)\n",
    "\n",
    "    # append to DF\n",
    "    sim1_results.append((pred, sim1_count_investments, sim1_gross_rev, sim1_fees, sim1_net_rev, sim1_fees_percentage,\n",
    "                         sim1_average_net_revenue, sim1_avg_investments_per_day, sim1_capital, sim1_CAGR))\n",
    "\n",
    "    # output for all predictions with some positive predictions\n",
    "    if sim1_count_investments > 1:\n",
    "        print(\n",
    "            f\"    Financial Result: \\n {new_df[filter_test_and_positive_pred][['sim1_gross_rev_' + pred_prefix, 'sim1_fees_' + pred_prefix, 'sim1_net_rev_' + pred_prefix]].sum()}\")\n",
    "        print(f\"        Count Investments in {yTrad} years (on TEST): {sim1_count_investments}\")\n",
    "        print(f\"        Gross Revenue: ${int(sim1_gross_rev)}\")\n",
    "        print(f\"        Fees (0.2% for buy+sell): ${int(-sim1_fees)}\")\n",
    "        print(f\"        Net Revenue: ${int(sim1_net_rev)}\")\n",
    "        print(f\"        Fees are {int(-investment * sim1_fees / sim1_gross_rev)} % from Gross Revenue\")\n",
    "        print(f\"        Capital Required : ${int(sim1_capital)} (Vbegin)\")\n",
    "        print(f\"        Final value (Vbegin + Net_revenue) : ${int(sim1_capital + sim1_net_rev)} (Vfinal)\")\n",
    "\n",
    "        print(\n",
    "            f\"        Average CAGR on TEST ({yTrad} years) : {np.round(sim1_CAGR, 3)}, or {np.round(100.0 * (sim1_CAGR - 1), 1)}% \")\n",
    "\n",
    "        print(f\"        Average daily stats: \")\n",
    "        print(f\"            Average net revenue per investment: ${np.round(sim1_net_rev / sim1_count_investments, 2)} \")\n",
    "        print(f\"            Average investments per day: {int(np.round(sim1_avg_investments_per_day))} \")\n",
    "        print(f\"            Q75 investments per day: {int(np.round(sim1_q75_investments_per_day))} \")\n",
    "        print('=============================================+')\n",
    "\n",
    "# results in a DataFrame from an Array\n",
    "columns_simulation = ['prediction', 'sim1_count_investments', 'sim1_gross_rev', 'sim1_fees', 'sim1_net_rev',\n",
    "                      'sim1_fees_percentage', 'sim1_average_net_revenue', 'sim1_avg_investments_per_day',\n",
    "                      'sim1_capital', 'sim1_CAGR']\n",
    "\n",
    "df_sim1_results = pd.DataFrame(sim1_results, columns=columns_simulation)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
